{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List, Optional, Union, Dict, Annotated, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, trim_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "from llm_chains import decomposition_chain, requires_decomposition, rephrase_chain, get_plan_chain, assign_chat_topic, memory_decision_chain, check_knowledge_base_chain\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from token_counter import tiktoken_counter\n",
    "\n",
    "from team_tools import tavily_search_tool, arxiv_search_tool, repl_tool\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.types import Command\n",
    "from pre_chat import examples\n",
    "load_dotenv()\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "llm = ChatGroq(model='llama-3.3-70b-versatile', temperature=1.0)\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=5984,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "trimmer_first = trim_messages(\n",
    "    max_tokens=1500,\n",
    "    strategy=\"first\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    # include_system=False,\n",
    "    allow_partial=True,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store, examples=examples)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles. optionally, align the search process based on pdf name (.pdf file) if given.\",\n",
    ")\n",
    "\n",
    "\n",
    "decomposer_chain = decomposition_chain(llm=llm)\n",
    "check_query_chain = requires_decomposition(llm=llm)\n",
    "rephraser_chain = rephrase_chain(llm=llm)\n",
    "planner_chain = get_plan_chain(llm=llm)\n",
    "assign_topic_chain = assign_chat_topic(llm=llm)\n",
    "check_knowledge_base = check_knowledge_base_chain(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = PostgresStore.from_conn_string(conn_string=\"postgresql://adi:root@localhost:5432/chat_store\")\n",
    "user_id = 'd36a9747-e419-4c20-b6ee-714be5fc3790'\n",
    "con_id = 'b653fa76-f021-413a-9b69-1fd561f31d07'\n",
    "conn_string = \"postgresql://adi:root@localhost:5432/chat_store\"\n",
    "\n",
    "\n",
    "def get_metadata_info(conn_string, user_id, con_id):\n",
    "    with PostgresStore.from_conn_string(conn_string=conn_string) as store:\n",
    "        # Search for metadata\n",
    "        conversation_metadata = store.search((\"conversation_metadata\", user_id, con_id))\n",
    "        \n",
    "        # Format metadata\n",
    "        metadata_info = \"\\n\".join(\n",
    "            f\"{item.key.replace('metadata_', '')} | \"\n",
    "            f\"{item.value.get('title', 'Unknown')} | \"\n",
    "            f\"{', '.join(item.value.get('authors', []))} | \"\n",
    "            f\"{item.value.get('description', 'Unknown')}\"\n",
    "            for item in conversation_metadata\n",
    "        )\n",
    "    return metadata_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
