{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Qdrant client.\n",
      "Collection 'aireas-cloud' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5372/2427583180.py:127: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  qdrant_retriever_tool = qdrant_retriever.as_tool(\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, List, Optional, Union, Dict, Annotated, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, trim_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# from team_tools import tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "from llm_chains import decomposition_chain, requires_decomposition, rephrase_chain, get_plan_chain, assign_chat_topic, memory_decision_chain\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from token_counter import tiktoken_counter\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview', temperature=0.0)\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"what is neural_networks.pdf talking about\",\n",
    "        {\n",
    "            \"query\": \"neural networks\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"neural_networks.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Can you tell me something about the file cancer_research_study.pdf?\",\n",
    "        {\n",
    "            \"query\": \"cancer research\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"cancer_research_study.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"I need to know what ethics_in_ai.pdf says on ethical concerns.\",\n",
    "        {\n",
    "            \"query\": \"ethical concerns\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"ethics_in_ai.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Find me anything related to quantum_computing_paper.pdf\",\n",
    "        {\n",
    "            \"query\": \"quantum computing\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"quantum_computing_paper.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Are there any references to Einstein's theories in physics_papers.pdf?\",\n",
    "        {\n",
    "            \"query\": \"Einstein's theories\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"physics_papers.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's in the climate_change_analysis.pdf about global warming?\",\n",
    "        {\n",
    "            \"query\": \"global warming\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"climate_change_analysis.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Show me papers discussing blockchain from blockchain_articles.pdf\",\n",
    "        {\n",
    "            \"query\": \"blockchain technology\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"blockchain_articles.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Can you retrieve sections of deep_learning_basics.pdf?\",\n",
    "        {\n",
    "            \"query\": \"deep learning\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"deep_learning_basics.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who authored machine_learning_review.pdf and neural_network_overview.pdf?\",\n",
    "        {\n",
    "            \"query\": \"authors\",\n",
    "            \"filter\": 'or(eq(\"pdf_name\", \"machine_learning_review.pdf\"), eq(\"pdf_name\", \"neural_network_overview.pdf\"))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Give me the details from data_analysis_guide.pdf\",\n",
    "        {\n",
    "            \"query\": \"data analysis\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"data_analysis_guide.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=5984,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "trimmer_first = trim_messages(\n",
    "    max_tokens=1500,\n",
    "    strategy=\"first\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    # include_system=False,\n",
    "    allow_partial=True,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store, examples=examples)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles. optionally, align the search process based on pdf name (.pdf file) if given.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_chain = decomposition_chain(llm=llm)\n",
    "check_query_chain = requires_decomposition(llm=llm)\n",
    "rephraser_chain = rephrase_chain(llm=llm)\n",
    "planner_chain = get_plan_chain(llm=llm)\n",
    "assign_topic_chain = assign_chat_topic(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = create_react_agent(llm, tools=[retriever_tool])\n",
    "\n",
    "\n",
    "# for s in k.stream({'messages': [HumanMessage('title of s.pdf')]}, stream_mode='values'):\n",
    "#   print(s['messages'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperMetadata(TypedDict):\n",
    "    title: Annotated[str, \"The title or heading of the research paper.\"]\n",
    "    authors: Annotated[List[str], \"The authors of the research paper.\"]\n",
    "    publish_date: Annotated[Optional[str], \"The publication date of the research paper in the format YYYY-MM-DD.\"]\n",
    "    description: Annotated[str, \"A concise description (2-3 sentences) summarizing the content of the research paper.\"]\n",
    "\n",
    "def basic_metadata_extraction_chain(llm):\n",
    "    \"\"\"\n",
    "    Creates a metadata extraction chain using a given language model.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to be used for metadata extraction.\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A chain that extracts the title, authors, publish date, and description from the text.\n",
    "    \"\"\"\n",
    "    # Template for the LLM\n",
    "    template = '''\n",
    "    You will be provided with the initial content of a research paper. Your task is to extract the following metadata accurately. \n",
    "    If you are uncertain about the answer or cannot find the information, populate the field with a suitable comment explaining why the information is unavailable.\n",
    "\n",
    "    The details to extract are as follows:\n",
    "    1. **Title of the paper:** Provide the exact title as it appears in the content. If the title is unclear, state \"Title not found in the provided content.\"\n",
    "    2. **List of authors:** Extract all authors listed. If no authors are mentioned, state \"Authors not mentioned in the provided content.\"\n",
    "    3. **Publication date:** Provide the date in the format YYYY-MM-DD. If no date is found, state \"Publication date not available.\"\n",
    "    4. **Description:** Summarize the paperâ€™s content in 2-3 concise sentences. If the description cannot be inferred, state \"Insufficient information to provide a description.\"\n",
    "\n",
    "    Use this structure to ensure clarity and completeness. If you need to make assumptions, mention them explicitly in the output.\n",
    "\n",
    "    Content:\n",
    "    {content}\n",
    "    '''\n",
    "\n",
    "    # Configure the LLM with structured output\n",
    "    structured_output_llm = llm.with_structured_output(PaperMetadata)\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', template),\n",
    "            # MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define additional processing steps if needed (e.g., trimming input text)\n",
    "    chain = prompt_template | trimmer_first | structured_output_llm\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "\n",
    "# bmc = basic_metadata_extraction_chain(llm=llm)\n",
    "\n",
    "\n",
    "# def extract_text_until_introduction(text):\n",
    "#     index = text.lower().find('introduction')\n",
    "#     if index != -1:\n",
    "#         return text[:index]\n",
    "#     else:\n",
    "#         return text\n",
    "\n",
    "# for f in os.listdir(os.path.join('test_dir')):\n",
    "#   print(f, '\\n')\n",
    "\n",
    "#   pdf_document = fitz.open(os.path.join('test_dir', f))\n",
    "\n",
    "#   text = \"\"\n",
    "  \n",
    "#   for page in pdf_document:\n",
    "#       text += page.get_text()\n",
    "#   pdf_document.close()\n",
    "\n",
    "#   extracted_text = extract_text_until_introduction(text)\n",
    "\n",
    "#   # print(extracted_text)\n",
    "\n",
    "#   result = bmc.invoke(extracted_text)\n",
    "#   print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "\n",
    "def process_files_with_chain(llm, chain, files):\n",
    "    \"\"\"\n",
    "    Process a list of PDF files, extract metadata using the given chain, and handle errors gracefully.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model used for metadata extraction.\n",
    "        chain: The chain to invoke for metadata extraction.\n",
    "        files: A list of file paths to process (from FastAPI or similar sources).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are file names and values are either metadata results or error messages.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            # Extract the file name\n",
    "            file_name = os.path.basename(file_path)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Open the PDF document\n",
    "            pdf_document = fitz.open(file_path)\n",
    "\n",
    "            # Extract text from the PDF\n",
    "            text = \"\"\n",
    "            for page in pdf_document:\n",
    "                text += page.get_text()\n",
    "            pdf_document.close()\n",
    "\n",
    "            # Extract text until the 'Introduction' section\n",
    "            extracted_text = extract_text_until_introduction(text)\n",
    "\n",
    "            # Invoke the chain to extract metadata\n",
    "            result = chain.invoke({\"content\": extracted_text})\n",
    "\n",
    "            # Store the result\n",
    "            results[file_name] = result\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any errors during processing\n",
    "            error_message = f\"Error processing {file_path}: {str(e)}\"\n",
    "            print(error_message)\n",
    "            results[file_name] = {\"error\": error_message}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from langgraph.store.memory import InMemoryStore\n",
    "# from langgraph.store.postgres import *\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# import langgraph\n",
    "\n",
    "\n",
    "# help(langgraph.store.postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata= {\n",
    "  \"s.pdf\": {\n",
    "    \"title\": \"EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation\",\n",
    "    \"authors\": [\n",
    "      \"Rang Meng\",\n",
    "      \"Xingyu Zhang\",\n",
    "      \"Yuming Li\",\n",
    "      \"Chenguang Ma\"\n",
    "    ],\n",
    "    \"publish_date\": \"Publication date not available\",\n",
    "    \"description\": \"This paper proposes EchoMimicV2, a half-body human animation method that simplifies unnecessary conditions and achieves striking animation quality by leveraging a novel Audio-Pose Dynamic Harmonization strategy. The method surpasses existing methods in both quantitative and qualitative evaluations.\"\n",
    "  },\n",
    "  \"sm.pdf\": {\n",
    "    \"title\": \"SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory\",\n",
    "    \"authors\": [\n",
    "      \"Cheng-Yen Yang\",\n",
    "      \"Hsiang-Wei Huang\",\n",
    "      \"Wenhao Chai\",\n",
    "      \"Zhongyu Jiang\",\n",
    "      \"Jenq-Neng Hwang\"\n",
    "    ],\n",
    "    \"publish_date\": \"Publication date not available.\",\n",
    "    \"description\": \"This paper introduces SAMURAI, an adaptation of the Segment Anything Model 2 for visual object tracking, which incorporates temporal motion cues to achieve robust and accurate tracking without retraining or fine-tuning. SAMURAI demonstrates strong zero-shot performance across diverse benchmark datasets.\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s.pdf': {'title': 'EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation',\n",
       "  'authors': ['Rang Meng', 'Xingyu Zhang', 'Yuming Li', 'Chenguang Ma'],\n",
       "  'publish_date': 'Publication date not available',\n",
       "  'description': 'This paper proposes EchoMimicV2, a half-body human animation method that simplifies unnecessary conditions and achieves striking animation quality by leveraging a novel Audio-Pose Dynamic Harmonization strategy. The method surpasses existing methods in both quantitative and qualitative evaluations.'},\n",
       " 'sm.pdf': {'title': 'SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory',\n",
       "  'authors': ['Cheng-Yen Yang',\n",
       "   'Hsiang-Wei Huang',\n",
       "   'Wenhao Chai',\n",
       "   'Zhongyu Jiang',\n",
       "   'Jenq-Neng Hwang'],\n",
       "  'publish_date': 'Publication date not available.',\n",
       "  'description': 'This paper introduces SAMURAI, an adaptation of the Segment Anything Model 2 for visual object tracking, which incorporates temporal motion cues to achieve robust and accurate tracking without retraining or fine-tuning. SAMURAI demonstrates strong zero-shot performance across diverse benchmark datasets.'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "user_id = '1'\n",
    "conversation_id = '1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in metadata.items():\n",
    "  # print(k, v)\n",
    "\n",
    "  store.put(('conversations_metadata', user_id, conversation_id), key=f'metadata_{k}', value=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'value': {'title': 'EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation',\n",
       "   'authors': ['Rang Meng', 'Xingyu Zhang', 'Yuming Li', 'Chenguang Ma'],\n",
       "   'publish_date': 'Publication date not available',\n",
       "   'description': 'This paper proposes EchoMimicV2, a half-body human animation method that simplifies unnecessary conditions and achieves striking animation quality by leveraging a novel Audio-Pose Dynamic Harmonization strategy. The method surpasses existing methods in both quantitative and qualitative evaluations.'},\n",
       "  'key': 'metadata_s.pdf',\n",
       "  'namespace': ['conversations_metadata', '1', '1'],\n",
       "  'created_at': '2024-12-14T10:51:47.560337+00:00',\n",
       "  'updated_at': '2024-12-14T10:51:47.560340+00:00',\n",
       "  'score': None},\n",
       " {'value': {'title': 'SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory',\n",
       "   'authors': ['Cheng-Yen Yang',\n",
       "    'Hsiang-Wei Huang',\n",
       "    'Wenhao Chai',\n",
       "    'Zhongyu Jiang',\n",
       "    'Jenq-Neng Hwang'],\n",
       "   'publish_date': 'Publication date not available.',\n",
       "   'description': 'This paper introduces SAMURAI, an adaptation of the Segment Anything Model 2 for visual object tracking, which incorporates temporal motion cues to achieve robust and accurate tracking without retraining or fine-tuning. SAMURAI demonstrates strong zero-shot performance across diverse benchmark datasets.'},\n",
       "  'key': 'metadata_sm.pdf',\n",
       "  'namespace': ['conversations_metadata', '1', '1'],\n",
       "  'created_at': '2024-12-14T10:51:47.560362+00:00',\n",
       "  'updated_at': '2024-12-14T10:51:47.560363+00:00',\n",
       "  'score': None}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = store.search(('conversations_metadata', user_id, conversation_id))\n",
    "\n",
    "n = [i.dict() for i in l]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
