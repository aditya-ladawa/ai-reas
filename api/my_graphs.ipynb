{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List, Optional, Union, Dict, Annotated, Literal\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, trim_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# from team_tools import tavily_search_tool, arxiv_search_tool, web_scraper_tool, repl_tool\n",
    "from qdrant_cloud_ops import initialize_selfquery_retriever, qdrant_vector_store\n",
    "from llm_chains import decomposition_chain, requires_decomposition, rephrase_chain, get_plan_chain, assign_chat_topic, memory_decision_chain, check_knowledge_base_chain\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from token_counter import tiktoken_counter\n",
    "\n",
    "from team_tools import tavily_search_tool, arxiv_search_tool, repl_tool\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "llm = ChatGroq(model='llama-3.2-90b-vision-preview', temperature=0.0)\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"what is neural_networks.pdf talking about\",\n",
    "        {\n",
    "            \"query\": \"neural networks\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"neural_networks.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Can you tell me something about the file cancer_research_study.pdf?\",\n",
    "        {\n",
    "            \"query\": \"cancer research\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"cancer_research_study.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"I need to know what ethics_in_ai.pdf says on ethical concerns.\",\n",
    "        {\n",
    "            \"query\": \"ethical concerns\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"ethics_in_ai.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Find me anything related to quantum_computing_paper.pdf\",\n",
    "        {\n",
    "            \"query\": \"quantum computing\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"quantum_computing_paper.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Are there any references to Einstein's theories in physics_papers.pdf?\",\n",
    "        {\n",
    "            \"query\": \"Einstein's theories\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"physics_papers.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's in the climate_change_analysis.pdf about global warming?\",\n",
    "        {\n",
    "            \"query\": \"global warming\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"climate_change_analysis.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Show me papers discussing blockchain from blockchain_articles.pdf\",\n",
    "        {\n",
    "            \"query\": \"blockchain technology\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"blockchain_articles.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Can you retrieve sections of deep_learning_basics.pdf?\",\n",
    "        {\n",
    "            \"query\": \"deep learning\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"deep_learning_basics.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who authored machine_learning_review.pdf and neural_network_overview.pdf?\",\n",
    "        {\n",
    "            \"query\": \"authors\",\n",
    "            \"filter\": 'or(eq(\"pdf_name\", \"machine_learning_review.pdf\"), eq(\"pdf_name\", \"neural_network_overview.pdf\"))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Give me the details from data_analysis_guide.pdf\",\n",
    "        {\n",
    "            \"query\": \"data analysis\",\n",
    "            \"filter\": 'eq(\"pdf_name\", \"data_analysis_guide.pdf\")',\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=5984,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "trimmer_first = trim_messages(\n",
    "    max_tokens=1500,\n",
    "    strategy=\"first\",\n",
    "    token_counter=tiktoken_counter,\n",
    "    # include_system=False,\n",
    "    allow_partial=True,\n",
    ")\n",
    "\n",
    "qdrant_retriever = initialize_selfquery_retriever(llm, qdrant_vector_store=qdrant_vector_store, examples=examples)\n",
    "qdrant_retriever_tool = qdrant_retriever.as_tool(\n",
    "    name=\"retrieve_research_paper_texts\",\n",
    "    description=\"Search and return information from the vector database containing texts of several research papers, and scholarly articles. optionally, align the search process based on pdf name (.pdf file) if given.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_chain = decomposition_chain(llm=llm)\n",
    "check_query_chain = requires_decomposition(llm=llm)\n",
    "rephraser_chain = rephrase_chain(llm=llm)\n",
    "planner_chain = get_plan_chain(llm=llm)\n",
    "assign_topic_chain = assign_chat_topic(llm=llm)\n",
    "check_knowledge_base = check_knowledge_base_chain(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(planner_chain.invoke({'task': \"explain depth first search and give the python code, also create a suumary report\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from tempfile import TemporaryDirectory\n",
    "# from typing import Dict, List, Optional\n",
    "# from langchain_experimental.utilities import PythonREPL\n",
    "# from typing_extensions import Annotated\n",
    "# from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# import logging\n",
    "\n",
    "# # Set up logging\n",
    "# # logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# WORKING_DIRECTORY = Path(\"test_dir\").resolve()\n",
    "# WORKING_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# def validate_file_name(file_name: str) -> bool:\n",
    "#     \"\"\"Validate file name to prevent path traversal attacks.\"\"\"\n",
    "#     safe_path = WORKING_DIRECTORY / file_name\n",
    "#     return safe_path.resolve().parent == WORKING_DIRECTORY.resolve()\n",
    "\n",
    "# @tool\n",
    "# def create_outline(\n",
    "#     points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "#     file_name: Annotated[str, \"File path to save the outline.\"],\n",
    "# ) -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "#     \"\"\"Create and save an outline.\"\"\"\n",
    "#     if not validate_file_name(file_name):\n",
    "#         return \"Error: Invalid file name or path.\"\n",
    "\n",
    "#     try:\n",
    "#         with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "#             for i, point in enumerate(points):\n",
    "#                 file.write(f\"{i + 1}. {point}\\n\")\n",
    "#         logging.info(f\"Outline successfully created: {file_name}\")\n",
    "#         return f\"Outline saved to {file_name}\"\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to create outline: {e}\")\n",
    "#         return f\"Error: Unable to create outline. Details: {e}\"\n",
    "\n",
    "# @tool\n",
    "# def read_document(\n",
    "#     file_name: Annotated[str, \"File path to read the document from.\"],\n",
    "#     start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "#     end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    "# ) -> str:\n",
    "#     \"\"\"Read the specified document.\"\"\"\n",
    "#     if not validate_file_name(file_name):\n",
    "#         return \"Error: Invalid file name or path.\"\n",
    "\n",
    "#     try:\n",
    "#         with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "#             lines = file.readlines()\n",
    "#         start = start or 0\n",
    "#         end = end or len(lines)\n",
    "#         return \"\\n\".join(lines[start:end])\n",
    "#     except FileNotFoundError:\n",
    "#         logging.error(f\"File not found: {file_name}\")\n",
    "#         return \"Error: File not found.\"\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to read document: {e}\")\n",
    "#         return f\"Error: Unable to read document. Details: {e}\"\n",
    "\n",
    "# @tool\n",
    "# def write_document(\n",
    "#     content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "#     file_name: Annotated[str, \"File path to save the document.\"],\n",
    "# ) -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "#     \"\"\"Create and save a text document.\"\"\"\n",
    "#     if not validate_file_name(file_name):\n",
    "#         return \"Error: Invalid file name or path.\"\n",
    "\n",
    "#     try:\n",
    "#         with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "#             file.write(content)\n",
    "#         logging.info(f\"Document successfully written: {file_name}\")\n",
    "#         return f\"Document saved to {file_name}\"\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to write document: {e}\")\n",
    "#         return f\"Error: Unable to write document. Details: {e}\"\n",
    "\n",
    "# @tool\n",
    "# def edit_document(\n",
    "#     file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
    "#     inserts: Annotated[\n",
    "#         Dict[int, str],\n",
    "#         \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "#     ],\n",
    "# ) -> Annotated[str, \"Path of the edited document file.\"]:\n",
    "#     \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "#     if not validate_file_name(file_name):\n",
    "#         return \"Error: Invalid file name or path.\"\n",
    "\n",
    "#     try:\n",
    "#         with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "#             lines = file.readlines()\n",
    "\n",
    "#         sorted_inserts = sorted(inserts.items())\n",
    "#         for line_number, text in sorted_inserts:\n",
    "#             if 1 <= line_number <= len(lines) + 1:\n",
    "#                 lines.insert(line_number - 1, text + \"\\n\")\n",
    "#             else:\n",
    "#                 return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "#         with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "#             file.writelines(lines)\n",
    "\n",
    "#         logging.info(f\"Document successfully edited: {file_name}\")\n",
    "#         return f\"Document edited and saved to {file_name}\"\n",
    "#     except FileNotFoundError:\n",
    "#         logging.error(f\"File not found: {file_name}\")\n",
    "#         return \"Error: File not found.\"\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to edit document: {e}\")\n",
    "#         return f\"Error: Unable to edit document. Details: {e}\"\n",
    "\n",
    "# repl = PythonREPL()\n",
    "\n",
    "# @tool\n",
    "# def python_repl_tool(\n",
    "#     code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "# ):\n",
    "#     \"\"\"Execute Python code and return the output.\"\"\"\n",
    "#     try:\n",
    "#         result = repl.run(code)\n",
    "#         logging.info(\"Python code executed successfully.\")\n",
    "#         return f\"Successfully executed:\\n```\\n{code}\\n```\\nStdout: {result}\"\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Python code execution failed: {e}\")\n",
    "#         return f\"Failed to execute. Error: {repr(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = PostgresStore.from_conn_string(conn_string=\"postgresql://adi:root@localhost:5432/chat_store\")\n",
    "user_id = 'd36a9747-e419-4c20-b6ee-714be5fc3790'\n",
    "con_id = 'b653fa76-f021-413a-9b69-1fd561f31d07'\n",
    "conn_string = \"postgresql://adi:root@localhost:5432/chat_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_info(conn_string, user_id, con_id):\n",
    "    with PostgresStore.from_conn_string(conn_string=conn_string) as store:\n",
    "        # Search for metadata\n",
    "        conversation_metadata = store.search((\"conversation_metadata\", user_id, con_id))\n",
    "        \n",
    "        # Format metadata\n",
    "        metadata_info = \"\\n\".join(\n",
    "            f\"{item.key.replace('metadata_', '')} | \"\n",
    "            f\"{item.value.get('title', 'Unknown')} | \"\n",
    "            f\"{', '.join(item.value.get('authors', []))} | \"\n",
    "            f\"{item.value.get('description', 'Unknown')}\"\n",
    "            for item in conversation_metadata\n",
    "        )\n",
    "    return metadata_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchTeamState(TypedDict):\n",
    "  messages: Annotated[List[BaseMessage], operator.add]    \n",
    "  task: str\n",
    "  plan_string: str\n",
    "  steps: List\n",
    "  results: dict\n",
    "  result: str\n",
    "\n",
    "def decompose_or_rephrase(state: ResearchTeamState):\n",
    "  question = state['messages'][-1].content\n",
    "  task = None\n",
    "  try:\n",
    "      status = check_query_chain.invoke(question)\n",
    "\n",
    "      if status == 'Decompose':\n",
    "          task = decomposer_chain.invoke(question)\n",
    "      elif status == 'Rephrase':\n",
    "          task = rephraser_chain.invoke(question)\n",
    "      else:\n",
    "          raise ValueError(f\"Unexpected status from check_query_chain: {status}\")\n",
    "  except Exception as e:\n",
    "      raise RuntimeError(f\"Error during processing: {e}\")\n",
    "\n",
    "  return {'task': task}\n",
    "\n",
    "\n",
    "def get_plan(state: ResearchTeamState):\n",
    "  regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "\n",
    "  task = state['task']\n",
    "  messages = state['messages']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  metadata_info = get_metadata_info(conn_string=conn_string, user_id=user_id, con_id=con_id)\n",
    "\n",
    "\n",
    "  rag_guidance = check_knowledge_base.invoke({'query': task, 'data': metadata_info})\n",
    "  plan = planner_chain.invoke({'task': task, 'knowledge_chain_answer': rag_guidance})\n",
    "\n",
    "  matches = re.findall(regex_pattern, plan)\n",
    "  return {\"steps\": matches, \"plan\": plan}\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "rag_agent = create_react_agent(llm, tools=[qdrant_retriever_tool])\n",
    "search_agent = create_react_agent(llm, tools=[tavily_search_tool])\n",
    "arxiv_agent = create_react_agent(llm, tools=[arxiv_search_tool])\n",
    "code_agent = create_react_agent(llm, tools=[repl_tool])\n",
    "\n",
    "\n",
    "def code_node(state: ResearchTeamState):\n",
    "    return {\"messages\": [HumanMessage(content=state['messages'][-1].content, name=\"Coder\")]}\n",
    "\n",
    "def retriever_node(state: ResearchTeamState):\n",
    "    return {\"messages\": [HumanMessage(content=state['messages'][-1].content, name=\"Retriever\")]}\n",
    "\n",
    "def search_node(state: ResearchTeamState):\n",
    "    return {\"messages\": [HumanMessage(content=state['messages'][-1].content, name=\"Searcher\")]}\n",
    "\n",
    "def arxiv_search_node(state: ResearchTeamState):\n",
    "    return {\"messages\": [HumanMessage(content=state['messages'][-1].content, name=\"ArXivSearcher\")]}\n",
    "\n",
    "def _get_current_task(state: ResearchTeamState):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "\n",
    "\n",
    "def agent_exec(state: ResearchTeamState):\n",
    "    \"\"\"Worker node that executes the agents accordingly for a given plan.\"\"\"\n",
    "\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    _step = _get_current_task(state)\n",
    "    step_desc, step_name, agent, agent_input = state[\"steps\"][_step - 1]\n",
    "\n",
    "    # Replace placeholders in agent_input with corresponding results\n",
    "    for k, v in _results.items():\n",
    "        agent_input = agent_input.replace(k, v) \n",
    "\n",
    "    # Dynamically select the agent function based on the agent name\n",
    "    if agent == \"RagSearcher\":\n",
    "        result = rag_agent.invoke(retriever_node(state))['messages'][-1].content\n",
    "    elif agent == \"Searcher\":\n",
    "        result = search_agent.invoke(search_node(state))['messages'][-1].content\n",
    "    elif agent == \"ChatBot\":\n",
    "        result = llm.invoke(agent_input)  # Assuming LLM invocation does not need message formatting\n",
    "    elif agent == \"Coder\":\n",
    "        result = code_agent.invoke(code_node(state))['messages'][-1].content\n",
    "    elif agent == \"ArXivSearcher\":\n",
    "        result = arxiv_agent.invoke(arxiv_search_node(state))['messages'][-1].content\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent}\")\n",
    "\n",
    "    if result is None:\n",
    "        raise ValueError(f\"Agent {agent} did not return a result for step {step_name}\")\n",
    "\n",
    "    # Store the result in the _results dictionary\n",
    "    _results[step_name] = str(result)\n",
    "\n",
    "    return {\"results\": _results}\n",
    "\n",
    "solve_prompt = \"\"\"\n",
    "We have created a detailed step-by-step Plan to solve the given task and obtained corresponding answers from agents for each step in the Plan. \n",
    "Use these agent-provided answers as Evidence to craft a clear, comprehensive, and cohesive response.\n",
    "\n",
    "Plan:  \n",
    "{plan}\n",
    "\n",
    "Using the Evidence from the answers provided for each step in the Plan, solve the given task:  \n",
    "Task: {task}\n",
    "\n",
    "Provide your response below in a well-structured and coherent format:  \n",
    "Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ResearchTeamState):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, agent, agent_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            agent_input = agent_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {agent}[{agent_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content, 'messages': [result]}\n",
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        return \"agent_exec\"\n",
    "graph = StateGraph(ResearchTeamState)\n",
    "\n",
    "graph.add_node('decompose_or_rephrase', decompose_or_rephrase)\n",
    "graph.add_node('get_plan', get_plan)\n",
    "graph.add_node(\"agent_exec\", agent_exec)\n",
    "graph.add_node(\"solve\", solve)\n",
    "\n",
    "graph.add_edge(START, 'decompose_or_rephrase')\n",
    "graph.add_edge('decompose_or_rephrase', 'get_plan')\n",
    "graph.add_edge('get_plan', 'agent_exec')\n",
    "graph.add_conditional_edges(\"agent_exec\", _route)\n",
    "graph.add_edge(\"solve\", END)\n",
    "\n",
    "research_graph_compiled = graph.compile(checkpointer=memory)\n",
    "research_graph = research_graph_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decompose_or_rephrase': {'task': 'what are the current encryption methods that are vulnerable to quantum attacks?, what are the existing post-quantum encryption algorithms?, how can these post-quantum encryption methods be integrated into current security protocols?'}}\n",
      "Non-relevant\n",
      "{'get_plan': {'steps': [('Search for current encryption methods vulnerable to quantum attacks, existing post-quantum encryption algorithms, and methods for integrating post-quantum encryption into current security protocols using the Searcher agent.', '#E1', 'Searcher', 'Current encryption methods vulnerable to quantum attacks, post-quantum encryption algorithms, integration into security protocols'), ('Summarize the findings on current encryption methods vulnerable to quantum attacks, existing post-quantum encryption algorithms, and methods for integrating post-quantum encryption into current security protocols based on the retrieved information.', '#E2', 'ChatBot', 'Summarize #E1'), ('Provide a detailed explanation on how to integrate post-quantum encryption methods into current security protocols based on the retrieved information.', '#E3', 'ChatBot', 'Explain integration of post-quantum encryption methods into security protocols #E1')]}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresearch_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat are the methods for post quantum encryption and security\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[32], line 82\u001b[0m, in \u001b[0;36magent_exec\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     80\u001b[0m _results \u001b[38;5;241m=\u001b[39m (state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     81\u001b[0m _step \u001b[38;5;241m=\u001b[39m _get_current_task(state)\n\u001b[0;32m---> 82\u001b[0m step_desc, step_name, agent, agent_input \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[43m_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Replace placeholders in agent_input with corresponding results\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _results\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'",
      "\u001b[0mDuring task with name 'agent_exec' and id 'a73e4da7-2461-0415-0b25-86e08f3e25fc'"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': '1'}}\n",
    "\n",
    "for s in research_graph.stream({'messages': [HumanMessage('what are the methods for post quantum encryption and security')]}, config=config):\n",
    "  print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM': 'Explain how gradient boosting machine works',\n",
       " 'documentation_team': 'Document important points about gradient boosting machine'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict, List, Literal, Dict\n",
    "\n",
    "# Define Worker roles\n",
    "Workers = Literal['research_team', 'documentation_team', 'LLM']\n",
    "\n",
    "# Prompt for supervisor managing teams with task assignments and dynamic sequence creation\n",
    "members = \", \".join(['research_team', 'documentation_team', 'LLM'])\n",
    "\n",
    "prompt = f'You are a supervisor managing the given teams: {members}' + '''\n",
    "Your task is to handle user queries by maintaining a seamless and cohesive conversation flow, ensuring that the user feels like they are interacting with a single agent, even when tasks are being delegated. \n",
    "\n",
    "Each worker will perform a task and respond with their results and status.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Casual or General Queries**: \n",
    "   - If the query is casual, general, or involves providing information that does not require specific investigation or documentation answer directly as the 'LLM'. \n",
    "     - **LLM Response**: Directly respond with the most relevant answer, utilizing your knowledge base.\n",
    "   \n",
    "2. **Task-Specific Queries**:\n",
    "   - **Research**: If the query involves research, new findings, or needs further investigation (e.g., \"Can you research anti-matter further?\"), the task should be delegated to the research team. **Do not** answer these types of questions with the LLM.\n",
    "   \n",
    "   - **Documentation**: If the query involves creation of new .txt files or manipulation of existing .txt files, the `documentation_team` should be involved.\n",
    "\n",
    "3. **Seamless Interaction**: \n",
    "   - When delegating a task, the response should be phrased in a way that makes it feel like a single agent is handling the request, without referring to multiple teams. For example:\n",
    "     - For research tasks: \"I'll gather more details on this topic for you.\"\n",
    "     - For documentation tasks: \"Let me prepare the document based on the latest research.\"\n",
    "\n",
    "4. **Context Handling**: \n",
    "   - You have access to the latest conversation history (limited by the context window) and should handle queries based on this context. \n",
    "   - Teams (research and documentation) do not have access to the conversation history, so it is the supervisor’s responsibility to track and respond accordingly, incorporating past messages to provide a cohesive conversation flow.\n",
    "\n",
    "5. **Maintain Context**: \n",
    "   - If the user is following up on a previous discussion (e.g., \"Did you find the research on anti-matter?\"), ensure the response feels like part of an ongoing conversation, keeping the user informed without mentioning team involvement. For example:\n",
    "     - \"I'm still gathering the latest research and will prepare the summary soon.\"\n",
    "\n",
    "6. **Proactive Engagement**: \n",
    "   - Feel free to initiate conversations or ask the user if they'd like more details on a topic based on prior discussions. This will help create a more cohesive experience.\n",
    "\n",
    "### Task Assignment:\n",
    "Please generate the task sequence and provide suitable tasks for each worker. Format the output as a dictionary with workers as keys and their tasks as values.\n",
    "\n",
    "### WARNINGS:\n",
    "1. The `documentation_team` should only be involved if the user **explicitly** requests document creation, file manipulation, or text editing. \n",
    "   **Do not** involve the `documentation_team` for general research or information gathering unless the user directly and clearly asks for it, such as requesting a report, summary, or document preparation. \n",
    "2. Ensure that the tasks are assigned in a logical, clear, and accurate order of execution.\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the SequenceCreator class to represent the sequence of workers and their tasks\n",
    "class SequenceCreator(TypedDict):\n",
    "    \"\"\"Defines the sequence of workers and their tasks.\"\"\"\n",
    "    passes: Dict[Workers, str]\n",
    "\n",
    "    def __init__(self, workers_tasks: Dict[Workers, str]):\n",
    "        # Simply assign the workers' tasks without adding 'FINISH'\n",
    "        self.passes = workers_tasks\n",
    "\n",
    "# Template for the chat\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Structured LLM output to map workers to tasks\n",
    "structured_llm_output = llm.with_structured_output(SequenceCreator)\n",
    "\n",
    "# Response chain to generate the task sequence and description\n",
    "response_chain = template | trimmer | structured_llm_output \n",
    "\n",
    "# Invoke response chain with the user query\n",
    "response = response_chain.invoke({\n",
    "    'messages': [\n",
    "\n",
    "    ],\n",
    "    'question': 'what do Xgradient boosting machine works, document imp points'\n",
    "})\n",
    "\n",
    "# Extracting the worker sequence and their tasks from the response\n",
    "worker_tasks = response['passes']  # Dictionary with worker as key and task as value\n",
    "\n",
    "# Output the task dictionary\n",
    "worker_tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal, Dict, TypedDict\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import HumanMessage, trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreState(TypedDict):\n",
    "  messages: Annotated[List[BaseMessage], operator.add]\n",
    "  passes: List[str]\n",
    "  call_next: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Workers = Literal['research_team', 'documentation_team', 'LLM']\n",
    "\n",
    "members = \", \".join(['research_team', 'documentation_team', 'LLM'])\n",
    "\n",
    "supervisor_prompt = f'You are a supervisor managing the given teams: {members}' + '''\n",
    "Your task is to handle user queries by maintaining a seamless and cohesive conversation flow, ensuring that the user feels like they are interacting with a single agent, even when tasks are being delegated. \n",
    "\n",
    "Each worker will perform a task and respond with their results and status.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Casual or General Queries**: \n",
    "   - If the query is casual, general, or involves providing information that does not require specific investigation or documentation answer directly as the 'LLM'. \n",
    "     - **LLM Response**: Directly respond with the most relevant answer, utilizing your knowledge base.\n",
    "   \n",
    "2. **Task-Specific Queries**:\n",
    "   - **Research**: If the query involves research, new findings, or needs further investigation (e.g., \"Can you research anti-matter further?\"), the task should be delegated to the research team. **Do not** answer these types of questions with the LLM.\n",
    "   \n",
    "   - **Documentation**: If the query involves creation of new .txt files or manipulation of existing .txt files, the `documentation_team` should be involved.\n",
    "\n",
    "3. **Seamless Interaction**: \n",
    "   - When delegating a task, the response should be phrased in a way that makes it feel like a single agent is handling the request, without referring to multiple teams. For example:\n",
    "     - For research tasks: \"I'll gather more details on this topic for you.\"\n",
    "     - For documentation tasks: \"Let me prepare the document based on the latest research.\"\n",
    "\n",
    "4. **Context Handling**: \n",
    "   - You have access to the latest conversation history (limited by the context window) and should handle queries based on this context. \n",
    "   - Teams (research and documentation) do not have access to the conversation history, so it is the supervisor’s responsibility to track and respond accordingly, incorporating past messages to provide a cohesive conversation flow.\n",
    "\n",
    "5. **Maintain Context**: \n",
    "   - If the user is following up on a previous discussion (e.g., \"Did you find the research on anti-matter?\"), ensure the response feels like part of an ongoing conversation, keeping the user informed without mentioning team involvement. For example:\n",
    "     - \"I'm still gathering the latest research and will prepare the summary soon.\"\n",
    "\n",
    "6. **Proactive Engagement**: \n",
    "   - Feel free to initiate conversations or ask the user if they'd like more details on a topic based on prior discussions. This will help create a more cohesive experience.\n",
    "\n",
    "### Task Assignment:\n",
    "Please generate the task sequence and provide suitable tasks for each worker. Format the output as a dictionary with workers as keys and their tasks as values.\n",
    "\n",
    "### WARNINGS:\n",
    "1. The `documentation_team` should only be involved if the user **explicitly** requests document creation, file manipulation, or text editing. \n",
    "   **Do not** involve the `documentation_team` for general research or information gathering unless the user directly and clearly asks for it, such as requesting a report, summary, or document preparation. \n",
    "2. Ensure that the tasks are assigned in a logical, clear, and accurate order of execution.\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: List[str], prompt_for_supervisor: str):\n",
    "  options = members + ['FINISH']\n",
    "  system_prompt = prompt_for_supervisor\n",
    "\n",
    "  class SequenceCreator(TypedDict):\n",
    "      \"\"\"Defines the sequence of workers and their tasks.\"\"\"\n",
    "      passes: Dict[Workers, str]\n",
    "      def __init__(self, workers_tasks: Dict[Workers, str]):\n",
    "          self.passes = workers_tasks\n",
    "\n",
    "  def supervisor_node(state) -> Command[Literal[*members,  \"__end__\"]]:\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "      ('system', system_prompt),\n",
    "      MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "\n",
    "    structured_llm_output = llm.with_structured_output(SequenceCreator)\n",
    "\n",
    "    response_chain = template | trimmer | structured_llm_output\n",
    "\n",
    "    return response_chain\n",
    "\n",
    "   #  response = response_chain.invoke({})\n",
    "\n",
    "   #  work_sequence = response['sequence']\n",
    "\n",
    "   #  next_worker = work_sequence[0]\n",
    "\n",
    "   #  if goto == \"FINISH\":\n",
    "   #      next_worker = END\n",
    "\n",
    "   #  return Command(update={'passes': work_sequence, 'call_next': next_worker}, goto=next_worker)\n",
    "  \n",
    "  return supervisor_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_node = make_supervisor_node(llm=llm, members=['research_team', 'documentation_team', 'LLM'], prompt_for_supervisor=supervisor_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge ending at unknown node `research_team`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m gp\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervisor\u001b[39m\u001b[38;5;124m'\u001b[39m, supervisor_node)\n\u001b[1;32m      4\u001b[0m gp\u001b[38;5;241m.\u001b[39madd_edge(START, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/graph/state.py:510\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[0;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    507\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[1;32m    519\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     ]\n\u001b[1;32m    528\u001b[0m )\n",
      "File \u001b[0;32m~/Aditya/z_projects/aireas/myenv/lib/python3.12/site-packages/langgraph/graph/graph.py:405\u001b[0m, in \u001b[0;36mGraph.validate\u001b[0;34m(self, interrupt)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m END:\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge ending at unknown node `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# validate interrupts\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupt:\n",
      "\u001b[0;31mValueError\u001b[0m: Found edge ending at unknown node `research_team`"
     ]
    }
   ],
   "source": [
    "gp = StateGraph(CoreState)\n",
    "gp.add_node('supervisor', supervisor_node)\n",
    "\n",
    "gp.add_edge(START, \"supervisor\")\n",
    "gp = gp.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
